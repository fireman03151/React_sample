---
title: Gradient Descent
localeTitle: أصل التدرج
---
## أصل التدرج

الانحدار التدرج هو خوارزمية التحسين للعثور على الحد الأدنى من وظيفة. في التعلم العميق ، تكون خوارزمية التحسين هذه مفيدة جدًا عندما لا يمكن حساب المعلمات تحليليًا.

![أصل التدرج](https://upload.wikimedia.org/wikipedia/commons/6/68/Gradient_descent.jpg) ما تريد القيام به هو تحديث قيمة المعلمة بشكل متكرر حتى تقوم بتقليل قيمة الدالة Cost J (θ) بالقرب من 0؛

### معدل التعليم

يسمى حجم الخطوة معدل التعلم. إن معدل التعلم الأكبر يجعل التكرار أسرع ولكن قد يتجاوز الحد الأدنى العالمي ، أي القيمة التي نبحث عنها. من ناحية أخرى ، يمكن أن نمنع هذا التجاوز عن طريق خفض معدل التعلم ؛ لكن احذر من أن أصغر معدل جعل التعلم ، وأكثر كثافة computationally يحصل عليه. قد يؤدي ذلك إما إلى إطالة الحساب دون داعٍ ، أو قد لا تصل إلى الحد الأدنى العالمي تمامًا.

### ميزة التحجيم

تتطلب منك مشكلة التعلم العميق استخدام ميزات متعددة لإنشاء نموذج تنبئي. على سبيل المثال ، إذا كنت تبني نموذجًا تنبئيًا لتسعير المنازل ، فسيتعين عليك التعامل مع ميزات مثل السعر نفسه وعدد الغرف ومنطقة اللوت ، وما إلى ذلك. وقد تختلف هذه الميزات إلى حد كبير في النطاق ، على سبيل المثال ، على سبيل المثال قد تتراوح مساحة المنطقة بين 0 و 2000 قدم مربع ، بينما تتراوح الميزات الأخرى مثل عدد الغرف بين 1 و 9.

هذا هو المكان الذي يتم فيه تحسين المقاييس ، والذي يُطلق عليه أيضًا اسم "التطبيع" ، للتأكد من أن خوارزمية التعلم الآلي تعمل بشكل صحيح.

### أصل الانحدار العشوائي

تتطلب مشكلات تعلم الآلة عادةً إجراء حسابات على حجم العينة بالملايين ، وقد يكون ذلك مكثفًا جدًا في الحساب.

في أصل التدرج العشوائي ، تقوم بتحديث المعلمة لتدرج التكلفة لكل مثال بدلاً من ذلك مجموع مجموع تدفقات التكلفة لكل الأمثلة. يمكنك الوصول إلى مجموعة من المعلمات الجيدة بشكل أسرع بعد مرور بضعة أمثلة فقط من خلال التدريب ، وبالتالي يصبح التعلم أسرع أيضًا.

### قراءة متعمقة

*   [دليل الشبكات العصبية والتعليم العميق](http://neuralnetworksanddeeplearning.com/)
*   [نزول التدرج لآلة التعلم](https://machinelearningmastery.com/gradient-descent-for-machine-learning/)
*   [الفرق بين أصل التدرج النحاسي ونحد الانحدار العشوائي](https://towardsdatascience.com/difference-between-batch-gradient-descent-and-stochastic-gradient-descent-1187f1291aa1)