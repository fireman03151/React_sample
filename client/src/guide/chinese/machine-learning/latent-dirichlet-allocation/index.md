---
title: Latent Dirichlet Allocation
localeTitle: 潜在的Dirichlet分配
---
在自然语言处理中，潜在Dirichlet分配（LDA）是一种生成统计模型，它允许未观察到的组解释观察集，解释为什么数据的某些部分是相似的。例如，如果观察是收集到文档中的单词，则假定每个文档是少量主题的混合，并且每个单词的创建可归因于文档的主题之一。 LDA是主题模型的示例。

假设您有以下句子：

我早餐吃了香蕉和菠菜冰沙 我喜欢吃西兰花和香蕉。 龙猫和小猫很可爱。 我姐姐昨天收养了一只小猫。 看看这只可爱的仓鼠嚼着一块西兰花。

Latent Dirichlet分配是一种自动发现这些句子所包含的主题的方法。例如，给定这些句子并询问2个主题，LDA可能会产生类似的东西

句子1和2：100％主题A. 句子3和4：100％主题B. 句子5：60％主题A，40％主题B. 主题A：30％西兰花，15％香蕉，10％早餐，10％咀嚼，......（此时，您可以将主题A解释为关于食物） 主题B：20％的龙猫，20％的小猫，20％的可爱，15％的仓鼠，......（此时，您可以将主题B解释为关于可爱的动物）

当然，问题是：LDA如何执行这一发现？

LDA模型

更详细地说，LDA将文档表示为以某些概率吐出单词的主题的混合。它假设文档以下列方式生成：在编写每个文档时，您

确定文档将具有的字数N（例如，根据泊松分布）。 为文档选择主题混合（根据固定的K主题集上的Dirichlet分布）。例如，假设我们上面有两个食物和可爱的动物主题，你可以选择由1/3食物和2/3可爱动物组成的文件。 通过以下方式生成文档中的每个单词： ...。首先选择一个主题（根据您在上面采样的多项分布;例如，您可以选择1/3概率的食物主题和2/3概率的可爱动物主题）。 ......然后使用主题生成单词本身（根据主题的多项分布）。例如，食物主题可能输出概率为30％的“西兰花”，概率为15％的“香蕉”，依此类推。

假设这个文档集合的生成模型，LDA然后尝试从文档中回溯以找到可能已经生成集合的一组主题。

例

我们举个例子吧。根据上面的过程，当你生成一些特定的文件D时，你可能会

决定D将是关于食物的1/2和关于可爱动物的1/2。 选择5为D中的单词数。 从食物主题中选择第一个单词，然后给出“西兰花”这个词。 选择第二个词来自可爱的动物主题，它给你“熊猫”。 选择第三个词来自可爱的动物话题，给你“可爱”。 选择第四个词来源于食物主题，给你“樱桃”。 从食物主题中选出第五个词，给你“吃”。

因此，在LDA模型下生成的文件将是“西兰花熊猫可爱的樱桃吃”（注意LDA是一个词袋模型）。

学习

所以现在假设你有一套文件。您已经选择了一些固定数量的K主题来发现，并希望使用LDA来学习每个文档的主题表示以及与每个主题相关联的单词。你怎么做到这一点？一种方式（称为崩溃的吉布斯采样\*）如下：

浏览每个文档，并将文档中的每个单词随机分配给K个主题之一。 请注意，此随机分配已经为您提供了所有主题的主题表示和所有主题的单词分布（尽管不是很好）。 所以要改进它们，为每个文件d ...... ......。浏览d中的每个单词w ... .......并且对于每个主题t，计算两件事：1）p（主题t |文档d）=文档d中当前分配给主题t的单词的比例，以及2）p（单词w |主题t） ）=对来自该单词w的所有文档的主题t的分配比例。重新分配wa新主题，你选择主题t与概率p（主题t |文件d）\* p（单词w |主题t）（根据我们的生成模型，这基本上是主题t生成单词w的概率，所以它有意义的是我们用这个概率重新采样当前单词的主题）。 （另外，我在这里讨论了一些事情，例如在这些概率中使用先验/伪计算。） .......换句话说，在这一步中，我们假设除了当前单词之外的所有主题分配都是正确的，然后使用我们的文档生成模型更新当前单词的赋值。 在重复上一步骤很多次后，您最终会达到一个大致稳定的状态，您的任务非常好。因此，使用这些分配来估计每个文档的主题混合（通过计算分配给该文档中每个主题的单词的比例）和与每个主题相关联的单词（通过计算分配给每个主题的单词的比例）。

Layman的解释

如果上面的讨论是一个小眼镜，这是另一种在不同领域看待LDA的方法。

假设你刚搬到一个新的城市。你是一个时髦人士和动漫迷，所以你想知道其他时尚人士和动漫爱好者往往会在哪里闲逛。当然，作为一个时髦人士，你知道你不能只是问，所以你做了什么？

这是一个场景：你在整个城镇范围内找出一堆不同的场所（文件），记下每个人中挂出的人（话语）（例如，爱丽丝在商场和公园里闲逛，鲍勃挂出来电影院和公园等等。至关重要的是，您不了解每个机构的典型利益集团（主题），也不了解每个人的不同利益。

因此，您需要选择一些K类别来学习（即，您想要了解人们所属的K类最重要的类别），并首先猜测为什么您会看到人们在哪里。例如，你最初猜测爱丽丝是在商场，因为对X有兴趣的人喜欢在那里闲逛;当你在公园看到她时，你猜是因为她对Y有兴趣的朋友喜欢在那里闲逛;当你在电影院看到鲍勃时，你随机猜测是因为这个城市的Z人真的很喜欢看电影;等等。

当然，你的随机猜测很可能是不正确的（毕竟它们是随机的猜测！），所以你想改进它们。这样做的一种方法是：

选择一个地方和一个人（例如，在商场的爱丽丝）。 为什么Alice很可能在商场？可能是因为同样兴趣的商场里的其他人给她发了一条消息告诉她要来。 换句话说，购物中心对X有兴趣的人越多，并且更强的Alice与兴趣X相关联（在她去的所有其他地方），Alice越有可能因为兴趣而在商场X。 因此，对Alice为什么在购物中心进行新的猜测，根据您认为的可能性选择一种可能性。

一遍又一遍地穿过每个地方和人。你的猜测一直在变得越来越好（毕竟，如果你注意到很多极客在书店里闲逛，你怀疑爱丽丝自己非常讨厌，那么爱丽丝在书店是个好消息，因为她的极客朋友告诉你她去那里;现在你已经更好地了解为什么爱丽丝可能在书店，你可以依次使用这些知识来改善你为什么其他人都在哪里的猜测，所以最终你可以停下来更新。然后拍摄猜测的快照（或多个快照），并使用它来获取您想要的所有信息：

对于每个类别，您可以计算分配到该类别的人员，以确定人们对此特别感兴趣的内容。通过观察人们自己，你也可以解释这个类别（例如，如果X类包含很多身穿运动衫并带着篮球运动的高个子人，你可能会将X解释为“篮球运动员”组）。 对于每个地点P和兴趣类别C，您可以计算P处的人员比例（因为当前的分配集合），这些给出了P的表示。例如，您可能会了解到挂起的人Barnes＆Noble的成员包括10％的赶时髦的人，50％的动漫迷，10％的运动员和30％的大学生。

真实世界的例子

最后，让我们来看一个真实世界的例子。我不久前将LDA应用于一组Sarah Palin的电子邮件（请参阅http://blog.echen.me/2011/06/27/ ...以获取博客文章或http://sarah-palin.heroku.com /对于允许您通过LDA学习的主题浏览电子邮件的应用程序，所以这里是算法学到的一些主题：

Trig /家庭/灵感：家庭，网站，邮件，上帝，儿子，从，祝贺，孩子，生活，孩子，向下，三角，婴儿，出生，爱，你，综合症，非常，特别，保佑，老，丈夫，年，谢谢，最好，...... 野生动物/ BP腐蚀：游戏，鱼，驼鹿，野生动物，狩猎，熊，极地，熊，生存，管理，地区，董事会，狩猎，狼，控制，部门，年，使用，狼，栖息地，猎人，驯鹿，程序，丹佛，钓鱼，...... 能源/燃料/石油/采矿：能源，燃料，成本，石油，阿拉斯加，价格，成本，诺姆，现在，高，正在，家庭，公共，电力，矿山，危机，价格，资源，需要，社区，费尔班克斯，回扣，使用，采矿，村庄，...... 天然气：天然气，石油，管道，agia，项目，自然，北方，生产商，公司，税，公司，能源，发展，坡，生产，资源，线，气体，transcanada，说，亿，计划，管理，百万，工业，...... 教育/废物：学校，废物，教育，学生，学校，百万，阅读，电子邮件，市场，政策，学生，年，高，新闻，国家，程序，第一次，报告，业务，管理，公告，信息，报告， 2008年，季度...... 总统竞选/选举：邮件，网络，来自，谢谢，你，盒子，麦凯恩，莎拉，非常，良好，伟大，约翰，希望，总统，真诚，伊斯拉，工作，保持，制造，添加，家庭，共和党，支持，做，宝，...

#### 推荐阅读：

*   https://en.wikipedia.org/wiki/Latent _Dirichlet_分配
*   http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/